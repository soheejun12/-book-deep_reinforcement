{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Tabular Learning and the Bellman Equation\n",
    "\n",
    "\n",
    "### 이번 장에서는.. \n",
    "#### - 이전의 크로스 엔트로피 방식보다 더 유연하고 강력한 RL방법인 \"Q-learning\"의 방식을 알아보자\n",
    "#### - FrozenLake 환경을 다시 사용하며, 새로운 개념(Q-learning)이 환경에 어떻게 적용되는지 보여주고, 환경의 불확실성(environment's uncertainty)을 어떻게 해결할 것이지 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-1 Value, state, and optimality \n",
    "\n",
    "## * 상태의 가치 \n",
    "<img src = \"./image/vs2.png\">\n",
    "\n",
    "    - 상태 s에서 얻을 수 있을거라 예상되는 보상의 합(총보상)\n",
    "    - $$r_t$$ : 에피소드 내에서 스텝 t에 대한 보상 \n",
    "- 총보상의 할인 여부는 사용자 결정 \n",
    "- 가치는 에이전트가 따르는 정책(policy)에 의해 계산 \n",
    "### - 예시 1\n",
    "<img src = \"./image/ex.png\">\n",
    "\n",
    "\n",
    "#### - 상태가 3개인 매우 간단한 예제 \n",
    "- <1> 에이전트의 초기 상태 \n",
    "- <2> \"left\" 액션 이후의 상태, 보상은 1.0 \n",
    "- <3> \"down\" 액션 이후의 상태, 보상은 2.0 \n",
    "   \n",
    "   \n",
    "#### - 환경은 항상 이산적(deterministic) \n",
    "- 모든 액션은 성공하며 항상 상태1에서 시작 \n",
    "- 상태2, 상태3에 도달하면 에피소드 종료 \n",
    "    \n",
    "    \n",
    "#### - 상태1의 가치는 무엇일까? \n",
    "- 에이전트의 행동에 대한 정보 즉, 정책에 대한 정보를 알아야 함 \n",
    "- 이런 간단한 환경이라도 에이전트는 무한대의 액션을 취할 수 있음 \n",
    "\n",
    "\n",
    "#### - 고려할 수 있는 정책 \n",
    "<img src = \"./image/ex.png\">\n",
    "\n",
    "- (1) 에이전트는 항상 왼쪽으로 간다 \n",
    "- (2) 에이전트는 항상 아래쪽으로 간다 \n",
    "- (3) 에이전트는 0.5의 확률로 왼쪽, 0.5의 확률로 아래쪽으로 간다 \n",
    "- (4) 에이전트는 10%의 경우에 왼쪽, 90%의 경우에 아래쪽으로 간다 \n",
    "    \n",
    "#### - 위의 정책들을 통한 상태1의 가치 계산 \n",
    "<img src = \"./image/ex.png\">\n",
    "\n",
    "- (1) 항상 왼쪽 : 1.0 : 왼쪽으로 가면 보상 1.0을 얻고 에피소드 종료 \n",
    "- (2) 항상 아래쪽 : 2.0 : 아래쪽으로 가면 보상 2.0을 얻고 에피소드 종료 \n",
    "- (3) 50% 왼쪽, 50% 아래쪽 : 1.5 = (1.0 * 0.5) + (2.0 * 0.5)\n",
    "- (4) 10% 왼쪽, 90% 아래쪽 : 1.9 = (1.0 * 0.1) + (2.0 * 0.9)\n",
    "\n",
    "#### - 가장 최적(optimal)의 정책은 무엇일까?\n",
    "- 강화학습에서의 목적은 최대한 많은 보상을 얻는 것 \n",
    "- 이런 one-step 환경에서 총보상은 상태1의 가치와 같음 \n",
    "- 그러므로 정책2\n",
    "\n",
    "#### - 하지만, 실제의 환경은 이렇게 단순하지 않음 \n",
    "- 실제 환경은 정책을 수식화하고 최적성을 증명하기 매우 어려움 \n",
    "\n",
    "\n",
    "### - 예시 2\n",
    "<img src =\"./image/ex2.png\">\n",
    "\n",
    "#### - 이전의 환경에서 확장된 환경 \n",
    "- 상태3의 변화\n",
    "    - 더이상 끝(terminal) 상태가 아님 \n",
    "    - -20의 보상과 함께 상태4로 이동됨 \n",
    "\n",
    "\n",
    "- 상태1에서 아래쪽으로 이동하면 나쁜 보상을 피할 수 없음 \n",
    "    - \"being greedy\"전략을 따르는 에이전트에게 함정 \n",
    "    \n",
    "#### - 확장된 환경에 대한 상태1의 가치 계산 \n",
    "- (1) 항상 왼쪽 : 1.0 \n",
    "- (2) 항상 아래쪽 : -18 = 2.0 - 20 \n",
    "- (3) 50% 왼쪽, 50% 아래쪽 : -8.5 = (1.0 * 0.5) + ((2.0 - 20) * 0.5)\n",
    "- (4) 10% 왼쪽, 90% 아래쪽 : -16.1 = (1.0 * 0.1) + ((2.0 - 20) * 0.9)\n",
    "\n",
    "#### - 가장 최적의 정책은 ?\n",
    "- 정책1 \n",
    "\n",
    "#### 이러한 예제들에서 \n",
    "- 단순하고(naive) 사소한(trivial) 환경을 통해 최적화 문제의 복잡성에 대해 알아봄 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
