{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value of action\n",
    "\n",
    "## Q-learning\n",
    "#### - $Q_s$$_,$$_a$ : 상태 s에서 액션 a를 취했을 때 얻을 수 있는 총보상 \n",
    "<img src =\"./image/ql.png\">\n",
    "\n",
    "#### - $Q_s$$_,$$_a$를 통해 $V_s$ 정의\n",
    "<img src =\"./image/ql2.png\">\n",
    "\n",
    "- 상태 s에서 가능한 모든 액션 a에 대해 $Q_s$$_,$$_a$의 최대값5\n",
    "\n",
    "\n",
    "#### - $Q_s$$_,$$_a$를 통해 $Q_s$$_,$$_a$ 정의\n",
    "<img src =\"./image/ql3.png\">\n",
    "\n",
    "- 현재 상태 s에서 액션 a를 취했을 때의 Q값 : 현재 보상 + 할인계수 * (다음 상태 s' 에서 얻을 수 있는 최대 Q값)\n",
    "\n",
    "### - 예시 \n",
    "<img src =\"./image/ex5.png\">\n",
    "\n",
    "#### frozen lake와 비슷한 환경 \n",
    "#### - 환경\n",
    "\n",
    "- 초기 상태 : $S_0$\n",
    "- 최종 상태(종료 상태) : $S_1$, $S_2$, $S_3$, $S_4$\n",
    "- 33%의 확률로 선택한 액션 수행 \n",
    "- 33%의 확률로 오른쪽으로 미끄러짐 \n",
    "- 33%의 확률로 왼쪽으로 미끄러짐 \n",
    "- 할인계수(gamma) = 1\n",
    "\n",
    "#### - 각 액션의 가치를 계산해보자 \n",
    "<img src = \"./image/ex52.png\">\n",
    "\n",
    "- 터미널 상태들의 Q값  = 0, 터미널 상태들의 가치는 즉시 보상의 값과 같음 \n",
    "    - 더이상 연결된 상태가 없기 때문에 (no outbound connection)\n",
    "    - $V_1$ = 1\n",
    "    - $V_2$ = 2\n",
    "    - $V_3$ = 3\n",
    "    - $V_4$ = 4\n",
    "    \n",
    "    \n",
    "- 액션의 가치 = (즉시 보상 + 차후(subsequent)상태들의 장기보상)의 예상합\n",
    "    \n",
    "    \n",
    "- 상태 $S_0$에서 액션들의 가치 (가는 곳 + 미끄러지는곳들) \n",
    "\n",
    "    - Q(S0, up) = 0.33 * $V_1$ + 0.33 * $V_2$ + 0.33 * $V_4$ = 0.33 * 1 + 0.33 * 2 + 0.33 * 4 = 2.31\n",
    "        \n",
    "    - Q(S0, left) = 0.33 * $V_2$ + 0.33 * $V_1$ + 0.33 * $V_3$ = 0.33 * 2 + 0.33 * 1 + 0.33 * 3 = 1.98\n",
    "        \n",
    "    - Q(S0, right) = 0.33 * $V_4$ + 0.33 * $V_1$ + 0.33 * $V_3$ = 0.33 * 4 + 0.33 * 1 + 0.33 * 3 = 2.64\n",
    "        \n",
    "    - Q(S0, down) = 0.33 * $V_3$ + 0.33 * $V_2$ + 0.33 * $V_4$ = 0.33 * 3 + 0.33 * 2 + 0.33 * 4 = 2.97\n",
    "    \n",
    "    \n",
    "- 상태 $S_0$의 최종 가치는 최대값인 2.97\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.31"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.33 * 1 + 0.33 * 2 + 0.33 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.98"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.33 * 2 + 0.33 * 1 + 0.33 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.33 * 4 + 0.33 * 1 + 0.33 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9699999999999998"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.33 * 3 + 0.33 * 2 + 0.33 * 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 현실에서, (In practice)\n",
    "#### - 에이전트는 V보다 Q에 대해 액션을 결정하는 것이 더 간단하기 떄문에 현실에서 더 편리함\n",
    "#### - Q의 경우,\n",
    "- 액션을 선택할 때, 에이전트는 현재상태에서 가능한 모든 액션에 대해 가장 큰 Q값을 가지는 액션을 선택\n",
    "\n",
    "#### -  V의 경우, \n",
    "- 에이전트는 가치뿐만 아니라 전이 확률까지 알아야 함 \n",
    "\n",
    "#### - 현실에서, 사전에 전이확률을 알 수 있는 경우는 드뭄 \n",
    "- 그러므로 에이전트는 모든 (액션, 상태) 쌍에 대해 전이확률을 추정해야함 \n",
    "\n",
    "#### - 챕터 뒤 쪽에서 Frozen Lake 환경에서 두가지 방법을 사용하여 문제를 해결해 볼 것 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
