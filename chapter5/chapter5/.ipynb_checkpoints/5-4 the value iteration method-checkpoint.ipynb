{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The value iteration method \n",
    "\n",
    "#### 전이에 있어 반복(loop)가 없었던 이전의 환경 \n",
    "- 끝 상태(terminal state)으로부터 중앙상태까지의 가치 계산이 가능 \n",
    "\n",
    "#### 하지만, 하나의 반복만으로도 환경에 대한 접근이 어려워짐 \n",
    "\n",
    "### - 예시 \n",
    "<img src =\"./image/ex6.png\">\n",
    "\n",
    "#### 두가지 상태가 있는 환경 \n",
    "#### 상태 $s_1$ 에서 시작하여 취할 수 있는 액션은 상태 $s_2$로 갈 수 있는 오직 한가지\n",
    "#### 보상은 2\n",
    "#### 상태 $s_2$ 에서 시작하여 취할 수 있는 액션은 상태 $s_1$로 갈 수 있는 오직 한가지\n",
    "#### 보상은 1\n",
    "\n",
    "#### 그러므로, 에이전트의 삶은 [$s_1$, $s_2$, $s_1$, $s_2$..] 의 반복\n",
    "\n",
    "#### 이러한 무한 반복을 피하기 위해 할인 계수 gamma = 0.9를 사용 \n",
    "\n",
    "####  이 때 두 상태의 가치는?\n",
    "- $s_1$, $s_2$로 부터의 모든 전이는 보상 2를 줌 \n",
    "- 모든 반대 전이는 보상 1를 줌 \n",
    "- 그러므로 우리의 보상은 [1, 2, 1, 2 ...] 의 연속 \n",
    "- 모든 상태에서 취할 수 있는 액션은 하나밖에 없기 때문에 에이전트는 선택의 여지가 없음 \n",
    "- 그러므로 우리는 수식으로 부터 최대값을 생략할 수 있음 \n",
    "- 모든 상태에서 가치는 무한합과 같음 \n",
    "<img src =\"./image/ex66.png\">\n",
    "<img src = \"./image/ex666.png\">\n",
    "\n",
    "- 사실, 상태들의 정확한 가치를 계산할 수는 없음\n",
    "- 하지만 할인계수 gamma를 통해 시간이 지남에 따라 기여도를 줄일 수 있음 \n",
    "    - gamma가 0.9 일때 \n",
    "    - 10스텝 이후에는 0.349 \n",
    "    - 100스텝 이후에는 0.0000266 \n",
    "    \n",
    "- 이를 통해 우리는 50번의 반복 이후에 멈추고도 꽤나 정확한 추정을 얻게됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.736450674121663"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#s2에서 s1\n",
    "sum([0.9**(2*i) + 2*(0.9**(2*i+1)) for i in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.262752483911719"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#s1에서 s2\n",
    "sum([2*(0.9**(2*i)) + (0.9**(2*i+1)) for i in range(50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가치 반복 알고리즘 (Value iteration algorithm)\n",
    "- 알려진 전이확률과 보상이 있는 MDP 환경에서 상태가치와 액션가치를 수치적으로 계산할 수 있는 알고리즘 \n",
    "\n",
    "\n",
    "- 상태 가치를 계산하는 과정 \n",
    "    - 1. 상태가치 $v_i$ 의 초기값 지정 (보통 0)\n",
    "    - 2. MDP의 모든 상태 s에 대해 벨만 업데이트 수행 \n",
    "    - 3. 2번째 단계를 충분한 수의 스텝만큼이나 더이상 변화가 없을 때까지 반복 \n",
    "    \n",
    "    \n",
    "- 액션가치(Q)를 계산하는 과정 \n",
    "    - 1. 모든 $Q_s$$_,$$_a$ 를 0으로 초기화\n",
    "    - 2. 모든 상태 s와 그 상태에서 수행할 수 있는 모든 액션 a에 대해 업데이트 수행 \n",
    "    - 3. 2번째 단계 반복\n",
    "    \n",
    "#### 실제 활용의 경우 \n",
    "- 몇가지 분명한 한계점이 존재 \n",
    "    - 환경의 상태 공간이 이산적이고 모든 상태에 대해 반복을 수행할 수 있을만큼 작아야 함 \n",
    "        - frozenlake 4x4, frozenlake 8x8 의 경우에는 가능하지만\n",
    "        - cartpole의 경우에는 분명하지 못함 \n",
    "            - cartpole의 관찰은 4개의 플롯값 \n",
    "        - 이러한 값의 작은 차이가 상태가치에 영향를 끼칠 수 있음 \n",
    "        - 해결방법은 관찰값을 이산화(discretization)시키는 것 \n",
    "        - cartpole의 관찰을 하나의 개별적인 상자(bin)으로 간주 하는 등 \n",
    "        - 하지만 여러 실용적인 문제들이 발생할 수 있음 \n",
    "            - 상자의 범위를 어디까지 할 섯인지, 가치 추정을 위해 얼만큼에 데이터를 사용할 것 인지 등 \n",
    "            \n",
    "            \n",
    "   - 실제 환경에서 액션의 전이확률과 보상행렬을 아는 경우는 드묾\n",
    "        - gym에서도 '상태, 액션을 취함으로써 가는 다음상태, 그에따른 보상' 만 알 수 있음 \n",
    "        - 알 수 있는 것은 환경과 에이전트 간의 상호작용의 기록 \n",
    "        - 하지만 벨만 업데이트에서, 우리는 보상과 전이확룰을 필요로 함\n",
    "        - 그러므로 에이전트의 경험을 우리가 알지 못하는 것(보상, 전이확률)을 추정하는데 사용해야 함 \n",
    "        - 그러기 위해서는 각 (상태, 다음상태, 액션)에 대한 횟수(counter)를 알아야 함\n",
    "  \n",
    "#### 이제 가치 반복이 frozenlake 환경에서 어떻게 작동하는지 살펴보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
