{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value iteration in practice\n",
    "\n",
    "#### Frozen lake 환경에서 value iteration 수행하기 \n",
    "\n",
    "## 주요 데이터 구조 \n",
    "#### - 보상 테이블(Reward table)\n",
    "- 각 상태에서 액션을 취해 다음 상태로 갔을 때 얻은 보상에 대한 사전\n",
    "- {상태, 액션, 다음상태 : 보상} \n",
    "\n",
    "#### - 전이 테이블(Transition table)\n",
    "- 각 상태에서 경험된 다음상태(전이)의 횟수(counter)에 대한 사전\n",
    "- 전이의 확률을 추정할 때 사용 \n",
    "- {상태, 액션 : {다음상태 : 횟수}}\n",
    "- 예)\n",
    "    - 상태0에서 액션1을 10번 수행했을때, 3번 수행하면 상태4, 7번 수행하면 상태5\n",
    "    - {(0, 1) : {4 : 3, 5 : 7}}\n",
    "    \n",
    "#### - 가치 테이블(Value table)\n",
    "- {상태 : 상태의 가치}\n",
    "\n",
    "\n",
    "## 전반적인 로직 \n",
    "#### 1) 환경에서 100번의 랜덤 스텝을 수행하여 보상테이블, 전이테이블의 값을 채움 \n",
    "#### 2) 모든 상태에 대해 가치반복을 수행하여 가치테이블 업데이트 \n",
    "#### 3) 업데이트된 가치테이블의 향상을 확인하기 위해 몇 개의 full 에피소드를 테스트로 수행 \n",
    "#### 3-1) 만약, 테스트 에피소드들의 평균 보상이 0.8 이상이면 학습 종료 \n",
    "#### 3-2) 테스트 에피소드를 수행하는 동안 환경으로 부터 얻은 모든 정보를 통해 보상테이블, 전이테이블 업데이트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Agent 클래스 \n",
    "- 학습 루프에서 사용할 테이블들과 함수들을 포함하는 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __init__() \n",
    "    - 환경 생성 \n",
    "    - 첫번째 관찰 얻음 \n",
    "    - 필요한 테이블들 생성 (보상, 전이, 가치)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent :\n",
    "    def __init__(self) :\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset() \n",
    "        self.rewards = collections.defaultdict(float) #key값이 없어도 에러 x\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- play_n_random_stpes(count) \n",
    "    - 환경으로부터의 랜덤 경험들을 수집\n",
    "    - 보상테이블, 전이테이블 업데이트 \n",
    "    - 학습을 위해 에피소드가 끝날 때까지 기다릴필요 없음 \n",
    "        - cross-entropy 방법과의 차이점 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def play_n_random_steps(self, count):\n",
    "        \n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample() #random experience\n",
    "            \n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            \n",
    "            #reward table \n",
    "            #{state, action, new state : reward}\n",
    "            self.rewards[(self.state, action, new_state)] = reward \n",
    "             \n",
    "            #transition table \n",
    "            #{state, action : {new state : count}}\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            \n",
    "            #is_done=True(에피소드가 종료되면)이면 새로운 관찰을 얻음 \n",
    "            #에피소드가 종료되지 않았으면 새로운 상태가 현재상태가 됨\n",
    "            self.state = self.env.reset() if is_done else new_state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- calc_action_value(state, action) \n",
    "    - 전이테이블, 보상테이블, 가치테이블을 통해 상태로부터의 액션가치 계산\n",
    "    - 목적 \n",
    "        - 상태에서 수행할 최적의 액션 선택\n",
    "        - 가치반복을 통해 새로운 상태가치 계산 \n",
    "        \n",
    "    - 과정 \n",
    "        - 전이테이블로부터 주어진 상태, 액션에 대한 전이 횟수 추출\n",
    "        - 전이 횟수를 모두 더해 상태에서 액션을 수행했을 때의 총 전이 횟수 계산 \n",
    "        - 총 전이 횟수와 각 상태로의 전이 횟수를 통해 전이확률 계산 \n",
    "        - 추정한 전이확률을 이용해서 벨만방정식을 통해 액션가치 계산 \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def calc_action_value(self, state, action) :\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        \n",
    "        action_values = 0.0\n",
    "        \n",
    "        #transits : {(state, action) : {new state : count}}\n",
    "        #target_counts : {new state : count}\n",
    "        for tgt_state, count in target_counts.items() :\n",
    "            reward = self.rewards[(state, action, tgt_state)]\n",
    "            action_value += (count/total) * (reward + GAMMA + self.values[tgt_state])\n",
    "            \n",
    "        return action_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- select_action(state) \n",
    "    - 주어진 상태에서 최적의 액션 결정 \n",
    "    - 모든 가능한 액션들에 대한 가치 계산 후 가장 큰 값을 가지는 액션을 선택 \n",
    "    - 에이전트는 가치추정에 대해 greedy하게 행동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(self, state):\n",
    "    best_action, best_value = None, None\n",
    "    \n",
    "    for action in range(self.env.action_space.n):\n",
    "        action_value = self.calc_action_value(state, action)#각 액션에대한 액션가치 계산\n",
    "        \n",
    "        if best_value is None or best_value < action_value :\n",
    "            best_value = action_value\n",
    "            best_action = action\n",
    "            \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- play_episode(env)\n",
    "    - select_action()을 사용해서 액션 선택\n",
    "    - 주어진 환경에 대해 하나의 full 에피소드 수행 \n",
    "    - 테스트 에피소드 수행에 사용 \n",
    "    - 하나의 에피소드를 위해 보상을 쌓음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env) :\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while True :\n",
    "        action = self.select_action(state) #best action\n",
    "        \n",
    "        new_state, reward, is_done, _ = env.step(action)\n",
    "        \n",
    "        self.rewards[(state, action, new_state)] = reward\n",
    "        self.transits[(state, action)][new_state] += 1\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        if is_done : \n",
    "            break  #에피소드가 종료되면 새로운 관찰을 받음 \n",
    "            \n",
    "        state = new_state\n",
    "        \n",
    "    return total_reward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value_iteration() \n",
    "    - 환경의 모든 상태에서 도달 가능한 모든 상태에 대한 가치 계산 \n",
    "    - 그 중 가장 큰 값으로 현재 상태가치 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(self):\n",
    "    for state in range(self.env.observation_space.n) :\n",
    "        state_values = [self.calc_action_value(state, action) \n",
    "                           for action in range(self.env.action_space.n)]#벨만방정식을 사용해서 가치계산 \n",
    "        \n",
    "        self.values[state] = max(state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "\n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for tgt_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, tgt_state)]\n",
    "            action_value += (count / total) * (reward + GAMMA * self.values[tgt_state])\n",
    "        return action_value\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def play_episode(self, env):\n",
    "        total_reward = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = new_state\n",
    "        return total_reward\n",
    "\n",
    "    def value_iteration(self):\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [self.calc_action_value(state, action)\n",
    "                            for action in range(self.env.action_space.n)]\n",
    "            self.values[state] = max(state_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  - main\n",
    "- 환경, 에이전트, 기록자 생성 \n",
    "- 100번의 랜덤 스텝\n",
    "- 모든 상태에 대해 가치반복 \n",
    "- 가치테이블을 정책으로 해서 테스트 에피소드 수행 \n",
    "    - 테스트 에피소드의 평균보상이 0.8 이상이면 학습 종료 \n",
    "- 텐서보드에 기록 (평균보상 추적, 학습루프 중지 조건 체크용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.050\n",
      "Best reward updated 0.050 -> 0.650\n",
      "Best reward updated 0.650 -> 0.800\n",
      "Best reward updated 0.800 -> 0.850\n",
      "Solved in 22 iterations!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\" :\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    \n",
    "    agent = Agent()\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"-v-iteration\")\n",
    "    \n",
    "    \n",
    "    iter_no = 0\n",
    "    best_reward = 0.0\n",
    "    \n",
    "    while True :\n",
    "        iter_no += 1\n",
    "        \n",
    "        agent.play_n_random_steps(100)\n",
    "        \n",
    "        agent.value_iteration()\n",
    "        \n",
    "        reward = 0.0\n",
    "        \n",
    "        for _ in range(TEST_EPISODES):\n",
    "            reward += agent.play_episode(test_env)\n",
    "            \n",
    "        reward /= TEST_EPISODES\n",
    "        \n",
    "        if reward > best_reward:\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            best_reward = reward\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 \n",
    "#### - 해결까지 12 ~ 100번 반복 \n",
    "#### - 실행의 80% 정도는 1초 안에 좋은 정책을 찾음 \n",
    "#### - cross-entropy 방법과 비교하면 매우 향상된 결과 \n",
    "#### - 성능 향상의 이유 1\n",
    "- 액션의 확률적 결과, 에피소드의 길이(평균 6~10)\n",
    "    - cross-entropy, 에피소드 내에서 어떤 스텝이 좋은건지 실수인지 파악을 못함 \n",
    "    - 가치 반복, 전이확률추론과 가치예측을 통해 각 상태의 개별적 가치와 액션의 확률적 결과를 포함\n",
    "    \n",
    "- 그러므로 가치반복이 더 간단하고 환경으로부터 더 적은 데이터를 요구함(space efficiency)\n",
    "\n",
    "#### - 성능 향상의 이유 2\n",
    "- 가치반복은 학습을 시작하기위해 full 에피소드가 필요하지 않음 \n",
    "- 하나의 데이터(example)을 가지고도 가치 업데이트 가능 \n",
    "- 하지만 frozen lake처럼 복잡한 보상 구조 (골에 가야만 1)을 가진 경우 최소한 하나의 성공적인 에피소드 필요 \n",
    "    - 8x8 frozen lake 환경의 경우\n",
    "        - 해결 때 까지 50 ~ 400 반복\n",
    "        - 대부분의 시간을 첫번째 성공적인 에피소드를 기다리는데 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float, {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter, {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.defaultdict(collections.Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
