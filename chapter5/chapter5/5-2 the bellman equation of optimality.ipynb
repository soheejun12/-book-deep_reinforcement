{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bellman equation of optimality\n",
    "\n",
    "### - 예시 1\n",
    "#### 약간은 추상적인(abstract) 접근 \n",
    "<img src =\"./image/ex3.png\">\n",
    "\n",
    "- 이산적인(deterministic) 경우 \n",
    "- 모든 액션은 100% 결과를 보장 \n",
    "\n",
    "\n",
    "#### 상황 \n",
    "- 에이전트가 상태 $s_0$을 관찰했다 가정\n",
    "- $s_0$에서 취할 수 있는 액션은 N개 \n",
    "- 모든 액션은 각각의 보상 $r_1$, ... $r_N$과 함께 다른 상태 $s_1$, ... $s_N$ 으로 이동 \n",
    "- 상태 $s_0$과 연결된 모든 상태에 대한 가치 $V_i$를 알고 있음 \n",
    "\n",
    "\n",
    "#### 이러한 상황에서 에이전트가 취할 수 있는 최적의 방침(course)는 ? \n",
    "- 액션 $a_i$를 취했을 때, 그 액션에 따른 상태 $s_0$의 가치 $V_0$\n",
    "<img src = \"./image/b1.png\">\n",
    "\n",
    "- 에이전트가 최적의 액션을 선택하기 위해서는, 모든 가능한 액션에 대한 결과를 계산하여 최대값을 선택\n",
    "<img src = \"./image/b2.png\">\n",
    "\n",
    "- 할인계수를 고려할 경우 \n",
    "<img src = \"./image/b3.png\">\n",
    "\n",
    "#### 벨만 방정식 (이산적인 경우)\n",
    "- 앞서봤던 greedy와 다르게, '즉각적인 보상 + 상태의 장기적인 가치(long-term value of state)'를 고려 \n",
    "- Bellman은 이를 통해 최적의 보상을 얻을 수 있음을 증명 \n",
    "\n",
    "\n",
    "#### 확률적인 경우(stochastic case) \n",
    "- 액션이 다른 상태에서 끝날 기회가 있는 확률적인 경우에 이러한 확장은 어렵지 않음 \n",
    "- 단순하게 다음 상태의 가치를 따라가는 것이 아니라 모든 가능한 액션에 대해 예상되는 가치를 계산 \n",
    "- 예시를 통해 살펴보자 \n",
    "\n",
    "### - 예시2\n",
    "#### 상태 $s_0$에서 수행할 수 있는 액션 $a_1$을 통해 이동할 수 있는 상태 3개\n",
    "<img src ='./image/ex4.png'>\n",
    "\n",
    "#### 상황\n",
    "- $s_0$에서 $a_1$을 취했을 때 $P_1$의 확률로 상태 $s_1$으로 갈 수 있음 \n",
    "- $s_0$에서 $a_1$을 취했을 때 $P_2$의 확률로 상태 $s_2$으로 갈 수 있음 \n",
    "- $s_0$에서 $a_1$을 취했을 때 $P_3$의 확률로 상태 $s_3$으로 갈 수 있음 \n",
    "\n",
    "\n",
    "- 각 상태들은 각각의 보상 $r_1$, $r_2$, $r_3$을 가짐\n",
    "\n",
    "#### 가치 계산 \n",
    "- 액션 $a_1$을 수행했을 때의 가치를 계산하기 위해서는 각 가치와 확률을 곱한 결과를 모두 더해야 함 \n",
    "<img src = \"./image/b11.png\">\n",
    "    - 각 상태에 대해 '확률 * (즉각보상 + 할인된 장기보상)' 들의 합\n",
    "    \n",
    "    \n",
    "- 좀 더 형식적으로, \n",
    "<img src =\"./image/b22.png\">\n",
    "    - p(a,0) -> \n",
    "\n",
    "- 벨만 방정식과 결합 \n",
    "<img src = \"./image/b33.png\">\n",
    "\n",
    "\n",
    "    - 상태의 최적의 가치 = 예상되는 (즉각적인 보상 + 다음 상태의 할인된 장기 보상)의 최대 기대값 \n",
    "    \n",
    "    - 공식은 회귀적(recursive) \n",
    "        - 상태의 가치는 즉각적으로 접근할 수 있는(reachable) 상태의 가치들에 듸해 결정 \n",
    "       \n",
    "    - 강화학습 뿐만 아니라 다른 분야 (dynamic programming..)에서도 널리 쓰임 \n",
    "    \n",
    "    - 이러한 가치는 최적의 보상 뿐만 아니라 최적의 정책을 알려줌 \n",
    "    \n",
    "    \n",
    "#### 결론 \n",
    "- 벨만 방정식 \n",
    "    - 에이전트가 종료되는 모든 상태에 대해, 에이전트는 최대의 보상을 줄 것이라 예상되는 액션을 수행해야 한다 \n",
    "    - 이 때의 보상은 즉각적인 보상과 한 단계(one-step) 할인된 장기보상의 합이다 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
