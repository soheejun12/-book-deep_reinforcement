{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. gradients parallelism \n",
    "#### : 여러 자식 process가 각각 학습하고, 학습 결과(gradients)를 마스터 process에 전달 \n",
    "- 여러 자식 process가 자신의 training data를 사용해 gradients 계산하고(학습), 그것을 마스터 process에 전달 \n",
    "- 마스터 process는 gradients를 모두 더하고 SGD 업데이트 수행\n",
    "\n",
    "<img src=\"./image/gradient.png\">\n",
    "\n",
    "- 여러 GPU들을 통해 수행할 수 있기 때문에, 더 확장성이 높음 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from lib import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "\n",
    "REWARD_STEPS = 4\n",
    "CLIP_GRAD = 0.1\n",
    "\n",
    "PROCESSES_COUNT = 4 #자식 process개수\n",
    "NUM_ENVS = 15\n",
    "\n",
    "GRAD_BATCH = 64 #각 자식 process에서 loss, gradients를 계산할 batch 개수  \n",
    "TRAIN_BATCH = 2 #SGD반복에 합쳐질 자식 process의 gradients batch 개수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 매 최적화 단계마다 TRAIN_BATCH * GRAD_BATCH (128)만큼의 학습 샘플이 사용됨 \n",
    "    - loss 계산, backpropagation은 꽤 무거운 연산임으로 효율성을 위해 GRAD_BATCH사용 \n",
    "    - 네트워크를 on-policy로 업데이트하는 것을 유지하기위해 TRAIN_BATCH를 작게 유지 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grads_func(proc_name, net, device, train_queue):\n",
    "#### - 자식 process에서 수행됨 \n",
    "#### - 넘겨주는 인자 \n",
    "- proc_name : process의 이름, Tensorboard writer 생성에 사용 \n",
    "- net : 공유 네트워크 \n",
    "- device : 연산을 수행할 device\n",
    "- train_queue : 계산된 gradients를 중앙 process에 전달할 때 사용할 queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grads_func(proc_name, net, device, train_queue):\n",
    "    \n",
    "    \"\"\"환경, 에이전트, 경험 생성\"\"\"\n",
    "    envs = [make_env() for _ in range(NUM_ENVS)]\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], device=device, apply_softmax=True)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    batch = []\n",
    "    frame_idx = 0\n",
    "    writer = SummaryWriter(comment=proc_name)#\n",
    "\n",
    "    \n",
    "    with common.RewardTracker(writer, stop_reward=REWARD_BOUND) as tracker:\n",
    "        with ptan.common.utils.TBMeanTracker(writer, batch_size=100) as tb_tracker:\n",
    "            \n",
    "            for exp in exp_source:\n",
    "                frame_idx += 1\n",
    "                new_rewards = exp_source.pop_total_rewards()\n",
    "                \n",
    "                if new_rewards and tracker.reward(new_rewards[0], frame_idx):\n",
    "                    break\n",
    "\n",
    "                batch.append(exp)\n",
    "                \n",
    "                if len(batch) < GRAD_BATCH:\n",
    "                    continue\n",
    "\n",
    "                states_v, actions_t, vals_ref_v = common.unpack_batch(batch, net, \n",
    "                                                                      last_val_gamma=GAMMA**REWARD_STEPS, \n",
    "                                                                      device=device)\n",
    "                batch.clear()\n",
    "\n",
    "                \n",
    "                net.zero_grad()#optimizer가 아닌 network\n",
    "                \n",
    "                logits_v, value_v = net(states_v)\n",
    "                loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n",
    "\n",
    "                log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "                adv_v = vals_ref_v - value_v.detach()\n",
    "                log_prob_actions_v = adv_v * log_prob_v[range(GRAD_BATCH), actions_t]\n",
    "                loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "                prob_v = F.softmax(logits_v, dim=1)\n",
    "                entropy_loss_v = ENTROPY_BETA * (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "\n",
    "                loss_v = entropy_loss_v + loss_value_v + loss_policy_v\n",
    "                loss_v.backward()\n",
    "\n",
    "                tb_tracker.track(\"advantage\", adv_v, frame_idx)\n",
    "                tb_tracker.track(\"values\", value_v, frame_idx)\n",
    "                tb_tracker.track(\"batch_rewards\", vals_ref_v, frame_idx)\n",
    "                tb_tracker.track(\"loss_entropy\", entropy_loss_v, frame_idx)\n",
    "                tb_tracker.track(\"loss_policy\", loss_policy_v, frame_idx)\n",
    "                tb_tracker.track(\"loss_value\", loss_value_v, frame_idx)\n",
    "                tb_tracker.track(\"loss_total\", loss_v, frame_idx)\n",
    "\n",
    "                # gather gradients\n",
    "                nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "                \n",
    "                grads = [param.grad.data.cpu().numpy() if param.grad is not None else None\n",
    "                         for param in net.parameters()]\n",
    "                #다음 반복에 변질(변형)될 것을 방지하기위해 개별 buffer에 gradients만 따로 저장 \n",
    "                \n",
    "                train_queue.put(grads) #train_queue에 저장되는 것은 gradients\n",
    "\n",
    "    #자식 process의 게임이 solved되면 큐에 None추가 \n",
    "    #reward가 REWARD_BOUND보다 크면 \n",
    "    train_queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method('spawn')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"Name of the run\")\n",
    "    args = parser.parse_args()\n",
    "    device = \"cuda\" if args.cuda else \"cpu\"\n",
    "    \n",
    "    \"\"\"환경, 네트워크 설정\"\"\"\n",
    "    env = make_env()\n",
    "    net = common.AtariA2C(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    net.share_memory()\n",
    "\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
    "\n",
    "    \"\"\"큐 생성, process생성\"\"\"\n",
    "    train_queue = mp.Queue(maxsize=PROCESSES_COUNT)\n",
    "    data_proc_list = []\n",
    "    for proc_idx in range(PROCESSES_COUNT):\n",
    "        proc_name = \"-a3c-grad_\" + NAME + \"_\" + args.name + \"#%d\" % proc_idx\n",
    "        data_proc = mp.Process(target=grads_func, args=(proc_name, net, device, train_queue))\n",
    "        data_proc.start()\n",
    "        data_proc_list.append(data_proc)\n",
    "        \n",
    "        \n",
    "\n",
    "    batch = []\n",
    "    step_idx = 0\n",
    "    grad_buffer = None\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            train_entry = train_queue.get() #gradients\n",
    "            \n",
    "            if train_entry is None: #게임이 solved된 것이면\n",
    "                break\n",
    "\n",
    "            \n",
    "            #게임이 solved된 것이 아니면 \n",
    "            step_idx += 1\n",
    "\n",
    "            \n",
    "            if grad_buffer is None:\n",
    "                grad_buffer = train_entry\n",
    "                \n",
    "            else:\n",
    "                for tgt_grad, grad in zip(grad_buffer, train_entry):\n",
    "                    tgt_grad += grad #train_entry의 gradients를 모두 더함 \n",
    "            \n",
    "            #TRAIN_BATCH(2)마다 \n",
    "            if step_idx % TRAIN_BATCH == 0:\n",
    "                for param, grad in zip(net.parameters(), grad_buffer):\n",
    "                    param.grad = torch.FloatTensor(grad).to(device) #더해진 gradients를 네트워크 파라미터의 grad로 배치 \n",
    "\n",
    "                nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "                optimizer.step() #배치된 gradient로 네트워크 업데이트 \n",
    "                grad_buffer = None\n",
    "                \n",
    "    #GPU자원에서 차지하는 좀비 process를 방지하기위해            \n",
    "    finally:\n",
    "        for p in data_proc_list:\n",
    "            p.terminate() #자식 process 종료 \n",
    "            p.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 표시되는 결과는 각 자식 process 의 local 값 (speed, 완료된 게임 개수)\n",
    "    - 자식 process 개수 만큼을 곱해야 전체 process의 값이 나옴 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./image/grad1.png\">\n",
    "<img src = \"./image/grad2.png\">\n",
    "<img src = \"./image/grad3.png\">\n",
    "<img src = \"./image/grad4.png\">\n",
    "<img src = \"./image/grad5.png\">\n",
    "<img src =\"./image/grad_final.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
