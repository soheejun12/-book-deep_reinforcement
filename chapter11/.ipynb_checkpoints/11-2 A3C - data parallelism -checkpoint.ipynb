{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A3C - data parallelism \n",
    "#### : 여러 자식 process가 환경과 상호작용하여 얻은 경험을 모아 하나의 main process가 학습 \n",
    "<img src = \"./image/data.png\">\n",
    "\n",
    "### 구현 \n",
    "#### - 간단함과 효율성을 위해 학습 process로부터의 신경망 가중치 전파는 구현되지 않음 \n",
    "- 직접 모으고 자식에게 가중치를 보내는 것 대신 모든 process의 네트워크는 공유됨 \n",
    "    - PyTorch의 내장 기능 \n",
    "    - 신경망 생성 시 **share_memory()** 를 호출하여 각 process의 모든 가중치에 같은 nn.Module 인스턴스를 사용\n",
    "    \n",
    "    - 0의 간접비를 가지는 것이 이 방법의 핵심, 성능 향상 가능 \n",
    "        - CUDA) GPU 메모리는 전체 host의 process간에 공유됨\n",
    "        - CPU) 프로세스 간 통신(IPC:Inter-Process Communication)을 사용할 경우 \n",
    "             \n",
    "    - **하지만** \n",
    "        - 예제 구현에서는 한 기계에서 하나의 GPU를 사용하여 학습하고 데이터를 모음 \n",
    "        - Pong 예제에서는 크게 문제 없지만, 더 큰 확장성을 원한다면 네트워크 가중치를 공유하는 부분 확장 필요 \n",
    "        \n",
    "### 주요 클래스 및 함수 \n",
    "#### - class Atari2C(nn.Module)\n",
    "- Actor-Critic 신경망 \n",
    "- 출력은 (policy, value) 튜플 \n",
    "\n",
    "#### - class RewardTracker\n",
    "- 전체(꽉 찬)(full) 에피소드의 할인된 보상 관리 \n",
    "- Tensorboard에 기록 \n",
    "- game 해결 여부 확인 \n",
    "\n",
    "#### - unpack_batch(batch, net , last_val_gamma)\n",
    "- n에피소드 step에 대한 transition 배치 (state, reward, action, last_state)를 학습에 적합하도록 변환 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pydoc.io/pypi/ptan-0.3/autoapi/agent/index.html?highlight=agent%20policyagent#agent.PolicyAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C로 pong게임하기 구현 \n",
    "- 이전 챕터들에서 봤던 코드와 거의 비슷 \n",
    "- 자식 프로세스 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import ptan \n",
    "import numpy as np\n",
    "import argparse\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp #python의 multiproecessing 패키지가 아님 주의\n",
    "\n",
    "from lib import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "REWARD_STEPS = 4\n",
    "CLIP_GRAD = 0.1\n",
    "\n",
    "PROCESSES_COUNT = 4 #자식 process 개수 (CPU core개수)\n",
    "#장치관리자 - 프로세스 확인가능 \n",
    "\n",
    "NUM_ENVS = 15 #각 자식 process가 사용할 환경 개수 \n",
    "\n",
    "#총 병렬 환경 개수 = 60 (15 * 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"환경 생성\"\"\" \n",
    "def make_env():\n",
    "    return ptan.common.wrappers.wrap_dqn(gym.make(ENV_NAME)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data_func (net, device, train_queue)\n",
    "#### - 자식 process 에서 실행됨 \n",
    "#### - train_queue \n",
    "- 자식 process가 master process로 보낼 데이터에 사용 \n",
    "- 생산자가 여럿, 소비자가 하나일 때 큐를 사용 \n",
    "- 두 가지 종류의 객체를 포함 \n",
    "    - TotalReward: 완료된 에피소드의 할인된 보상인 소수값의 reward 필드만을 가진 객체 \n",
    "    - ptan.experience.ExperienceSourceFirstLast()\n",
    "        - REWARD_STEPS만큼의 연속에서 (첫번째 상태, 첫번째 상태에서 취한 액션, 보상, 마지막 상태)를 가지는 객체 \n",
    "        - 학습에 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TotalReward = collections.namedtuple('TotalReward', field_names=\"reward\")\n",
    "\n",
    "\"\"\"총 에피소드 보상을 main 학습 process에 전달\"\"\"\n",
    "def data_func(net, device, train_queue):\n",
    "    envs = [make_env() for _ in range(NUM_ENVS)] #15 \n",
    "    \n",
    "    \n",
    "    agent = ptan.agent.PolicyAgent(lambda x: net(x)[0], device=device, apply_softmax=True)\n",
    "    #net(x)의 출력은 튜플 (policy, value) \n",
    "    #ptan.agent.PolicyAgent(model): 모델로부터 액션 확률분포를 얻고 그로부터 액션 선택 \n",
    "    \n",
    "    \n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "    #모든 trajectory 조각(전체 에피소드의 steps_count만큼 씩) 에 대해 할인된 보상 계산 \n",
    "    #(첫번째 상태, 첫번째 상태에서 취한 액션, 보상, 마지막 상태) 반환 \n",
    "\n",
    "\n",
    "    for exp in exp_source:\n",
    "        new_rewards = exp_source.pop_total_rewards()##\n",
    "        \n",
    "        #보상이 0(False)이 아닐때 \n",
    "        if new_rewards:\n",
    "            train_queue.put(TotalReward(reward=np.mean(new_rewards))) \n",
    "            #한 에피소드 조각들의 총 보상들에 대한 평균을 저장 \n",
    "        \n",
    "        train_queue.put(exp) \n",
    "        #train_queue에 총보상평균, 경험 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def why(a):\n",
    "    k = []\n",
    "    if a :\n",
    "        k.append(1)\n",
    "        \n",
    "    k.append(2)\n",
    "    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "why(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    mp.set_start_method('spawn') #PyTorch에서는 best option \n",
    "\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--cuda\", default=False, action=\"store_true\", help=\"Enable cuda\")\n",
    "    parser.add_argument(\"-n\", \"--name\", required=True, help=\"name of the run\") \n",
    "    #tensorboard에 표시될 이름 \n",
    "    args = parser.parse_args()\n",
    "    device = \"cuda\" if args.cuda else \"cpu\"\n",
    "    \n",
    "    writer = SummaryWriter(comment=\"-a3c-data_\" + NAME + \"_\" + args.name)\n",
    "    \n",
    "    \n",
    "    \"\"\"환경, 네트워크, 최적화 생성\"\"\"\n",
    "    env = make_env()\n",
    "    \n",
    "    net = common.AtariA2C(env.observation_space.shape, env.action_space.n).to(device) #device로 이동 \n",
    "    net.share_memory() #가중치 공유 요청 #GPU는 default로 지원, CPU는 사용 필요 \n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE, eps=1e-3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"자식 process가 메인 process에 전달할 데이터 큐\"\"\"\n",
    "    train_queue = mp.Queue(maxsize=PROCESSES_COUNT) #꽉찬 큐에는 새로 입력 불가능 (for on-policy)\n",
    "    data_proc_list = [] #data_process_list\n",
    "    \n",
    "    for _ in range(PROCESSES_COUNT): #4\n",
    "        data_proc = mp.Process(target=data_func, args=(net, device, train_queue))\n",
    "        #mp.Process(target, args): args를 인자로 target함수에 전달하여 실행\n",
    "        #함수와 인자를 할당하여 Process 객체 생성 \n",
    "        \n",
    "        data_proc.start() #data_fun()이 자식 process에서 실행 #set_start_\n",
    "        data_proc_list.append(data_proc) \n",
    "        \n",
    "        #train_queue에 각 자식 process의 데이터 저장 \n",
    "        \n",
    "    \n",
    "    \"\"\"학습\"\"\"\n",
    "    batch = []\n",
    "    step_idx = 0\n",
    "    \n",
    "    \n",
    "    try : \n",
    "        with common.RewardTracker(writer, stop_reward=REWARD_BOUND) as tracker:\n",
    "            with ptan.common.utils.TBMeanTracker(writer, batch_size=100) as tb_tracker:\n",
    "                \n",
    "                while True:\n",
    "                    train_entry = train_queue.get() #소비자 \n",
    "                    \n",
    "                    #queue에 있는게 reward\n",
    "                    if isinstance(train_entry, TotalReward):\n",
    "                        if tracker.reward(train_entry.reward, step_idx): \n",
    "                            #학습데이터의 보상이 REWARD_BOUND(18)보다 크면 사용 안함 \n",
    "                            break\n",
    "                            \n",
    "                        continue #작으면 \n",
    "                      \n",
    "                    step_idx += 1 \n",
    "                    batch.append(train_entry) #batch에는 exp객체가 저장 \n",
    "                    \n",
    "                    #batch가 쌓일때까지 \n",
    "                    if len(batch) < BATCH_SIZE:\n",
    "                        continue \n",
    "                        \n",
    "                    \n",
    "                    #batch가 다 쌓이면 \n",
    "                    #gamma**4 \n",
    "                    states_v, actions_t, vals_ref_v = common.unpack_batch(batch, net, last_val_gamma=GAMMA**REWARD_STEPS, device=device)\n",
    "                    #unpack_batch(batch, net, last_val_gamma): 환경 transition batch를 3개의 tensor로 반환 \n",
    "                                            ##(states batch, taken actions batch, Q-values batch)\n",
    "                    \n",
    "                    batch.clear()\n",
    "                    \n",
    "                    \n",
    "                    \"\"\"최적화\"\"\"\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    logits_v, value_v = net(states_v) #(policy, value)\n",
    "                    \n",
    "                    loss_value_v = F.mse_loss(value_v.squeeze(-1), vals_ref_v)\n",
    "                    \n",
    "                    log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "                    \n",
    "                    \n",
    "                    adv_v = vals_ref_v - value_v.detach()\n",
    "                    \n",
    "                    \n",
    "                    log_prob_actions_v = adv_v * log_prob_v[range(BATCH_SIZE), actions_t]\n",
    "                    \n",
    "                    loss_policy_v = -log_prob_actions_v.mean()\n",
    "                    \n",
    "                    prob_v = F.softmax(logits_v, dim=1)\n",
    "                    \n",
    "                    entropy_loss_v = ENTROPY_BETA * (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "                    \n",
    "                    loss_v = entropy_loss + loss_value_v + loss_policy_v \n",
    "                    loss_v.backward()\n",
    "                    \n",
    "                    nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    \"\"\"tensorboard\"\"\"\n",
    "                    tb_tracker.track(\"advantage\", adv_v, step_idx)\n",
    "                    tb_tracker.track(\"values\", value_v, step_idx)\n",
    "                    tb_tracker.track(\"batch_rewards\", vals_ref_v, step_idx)\n",
    "                    tb_tracker.track(\"loss_entropy\", entropy_loss_v, step_idx)\n",
    "                    tb_tracker.track(\"loss_policy\", loss_policy_v, step_idx)\n",
    "                    tb_tracker.track(\"loss_value\", loss_value_v, step_idx)\n",
    "                    tb_tracker.track(\"loss_total\", loss_v, step_idx)\n",
    "    \n",
    "    #예외 발생, 게임 해결 시 실행\n",
    "    finally:\n",
    "        for p in data_proc_list:\n",
    "            p.terminate() #자식 process 종료 \n",
    "            p.join() #동기화\n",
    "            \n",
    "            #남은 process가 없도록 \n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "<img src=\"./image/data1.png\">\n",
    "<img src=\"./image/data2.png\">\n",
    "<img src=\"./image/data3.png\">\n",
    "<img src=\"./image/data4.png\">\n",
    "<img src=\"./image/data5.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수렴 역학 면에서의 결과는 A2C에 병렬환경을 사용한 것과 비슷하지만 속도가 더 빠름 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
