{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chapter 11. Asynchoronous Advantage Actor Critic\n",
    "#### : (A3C) 이전 챕터 A2C + 실제 비동기적 환경 상호작용\n",
    "\n",
    "## 1. Correlation and sample efficiency \n",
    "### * sample간의 상관관계는 SGD  최적화에서 매우 중요한 독립과 동등 분산 가정(i.i.d) 침해 \n",
    "- 학습 배치에 비슷한 samlple이 많으면 \n",
    "    - 네트워크가 같은(한) 방향으로 가게됨 \n",
    "    - 그 sample이 그저 운이 좋거나 나쁜 경우라면 잘못된 방향으로 갈 수 있음  \n",
    "    \n",
    "#### DQN, \n",
    "- replay buffer에 데이터(이전 상태들)를 저장하고 random sampling하여 학습 데이터로 사용하여 해결 \n",
    "\n",
    "#### 하지만, 이 방법은 PG(Policy Gradients) 방법들에서 사용 불가능 \n",
    "- 대부분의 PG방법은 on-policy \n",
    "    - 현재 정책(current policy)에 의해 생성된 sample에 대해서 학습해야함 \n",
    "    - remembering old transition은 더이상 불가능 \n",
    "    \n",
    "    \n",
    "- 시도할 수는 있지만, PG의 결과는 sample 생성에 사용된 policy\n",
    "    - 업데이트하고 싶은 현재 policy가 아님 \n",
    "    \n",
    "    \n",
    "### 가장 흔하게 사용되는 해결방법\n",
    "#### - 여러 병렬(parallel) environment을 통해 현재 policy로 탐색된 transition 모으기 \n",
    "- 각 다른 환경에서 얻은 여러 에피소드로 학습하기 때문에 샘플간 상관관계가 없음 \n",
    "- 동시에, 현재 policy를 사용할 수 있음 \n",
    "- **하지만** 샘플 효율성(sample efficency)이 좋지 않음 \n",
    "- 한 번 학습 후 얻은 모든 경험을 버림  \n",
    "    - PG, fresh data를 사용하기 때문에 환경에서 얻은 경험은 1번만 사용됨 \n",
    "    - PG는 value-based / off-policy 방법보다 데이터 효율성이 낮음 \n",
    "   \n",
    "   \n",
    "    - 상황에 따라 방법을 선택\n",
    "        - 환경과 에이전트간 상호작용 비용(속도, 메모리 차지 공간, 병렬 허용 등)이 작다면, PG\n",
    "        - 환경과 에이전트간 상호작용 비용이 크다면, value-based\n",
    "\n",
    "\n",
    "## 2. Adding an extra A to A2C\n",
    "### 이전 챕터에서 본 A2C \n",
    "<img src=\"./image/a2c.png\">\n",
    "\n",
    "- 수행은 병렬적이지만 소통은 병렬적이지 않음 \n",
    "- 병렬 환경으로부터 얻은 sample들은 ExperienceSource클래스에 섞여서 저장됨 \n",
    "- 잘 작동하지만 자원 운영 계산(computing resource utilization)면에서는 완벽하지않음 \n",
    "- 완전 병렬적으로 해보자 \n",
    "\n",
    "### 병렬 프로그래밍 (parallel programming)\n",
    "- 하드웨어 \n",
    "    - 요즘은 보통 작업환경에서도 여러 cpu core를 가지는 경우가 많음 \n",
    "    \n",
    " \n",
    "- 소프트웨어 \n",
    "    - 전통적 프로그래밍보단 병렬 프로그래밍이 더 어려움 \n",
    "    - 제 3의 라이브러리들을 통해 표현력과 유연성이 좋은 python을 통해 쉽게 가능 \n",
    "    - PyTorch 자체에서 병렬 프로그래밍 지원 \n",
    "        - torch.multiprocessing \n",
    " \n",
    "## actor-critic 병렬에 관한 두가지 접근법 (종류)\n",
    "### 1. Data parallelism\n",
    "<img src = \"./image/data.png\">\n",
    "\n",
    "- 여러 환경과 소통하는 여러 process들은 transition $(s, r, a, s')$을 제공\n",
    "- 이 sample들은 하나의 학습 process로 모아져 손실계산, SGD 업데이트 수행 \n",
    "- 업데이트된 신경망 파라미터는 각 process에 전파됨 \n",
    "- 전파된 파라미터를 통해 다시 환경과 소통 \n",
    "\n",
    "\n",
    "### 2. Gradients parallelism \n",
    "<img src=\"./image/gradient.png\">\n",
    "\n",
    "- 여러 환경과 소통하는 여러 process들은 각각의 sample에 대해 gradient 계산 \n",
    "- 계산된 gradient를 모두 합쳐 하나의 process에서 SGD 업데이트 수행 \n",
    "- 업데이트된 신경망 가중치는 모든 worker에 전파됨 \n",
    "    - data on-policy 유지\n",
    "    \n",
    "    \n",
    "#### - 두 방법의 computation cost 차이 \n",
    "- A3C에서 계산비용이 가장 큰 연산은 trainig process\n",
    "    - 데이터 손실 계산, 손실에 따른 gradients 계산 \n",
    "    \n",
    "- SGD는 비교적 연산이 적음 \n",
    "    - 네트워크 가중치에 scaled 가중치 더하기\n",
    "    \n",
    "    \n",
    "#### - 자원 및 목적에 따라 적합한 방법 사용 \n",
    "- 최적화할 문제가 1개, GPU 여러개, 여러 기계 --> Gradient parallelism \n",
    "    - 더 빠른 학습 가능 \n",
    "    \n",
    "    \n",
    "- GPU 1개  --> Data parallelism \n",
    "    - 더 구현하기 쉬움 \n",
    "\n",
    "\n",
    "#### - 이번 장에서는, \n",
    "- Pong게임을 통해 두 방법(data parallelism, gradients parallelism) 구현 및  비교 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiprocessing in Python \n",
    "#### -Python의 multiporcessing (mp) 모듈 \n",
    "- process-level 병렬을 지원\n",
    "- communication 기초요소(primitives) 요구 \n",
    "- 예제에서 사용할 클래스 2개 \n",
    "- **mp.Queue**\n",
    "    - 동시(concurrent) 멀티 생산자(multi-producer)\n",
    "    - 명백한 직렬화를 가진 FIFO(First In First Out) 큐 멀티 소비자(multi-consumer)\n",
    "    - 큐 안에 있는 객체의 역직렬화 \n",
    "    \n",
    "    \n",
    "- **mp.Process**\n",
    "    - 자식 process에서 돌아가는 코드 조각\n",
    "    - 그것을 부모 process에서 제어 가능하게 해줌 \n",
    "    \n",
    "\n",
    "#### - PyTorch에서 mp 모듈과 같은 기능을 하는 자체 wrapper 제공 \n",
    "- CUDA 장치와 공유 메모리의 텐서 및 변수 다루는 기능 추가 \n",
    "- import multiprocessing 대신 **torch.multiprocessing** 을 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
