{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov decision processes\n",
    "- 강화학습의 가장 기본적인 이론 \n",
    "- Markov process, Markov reward process, Markov decision process\n",
    "\n",
    "### 1. Markov process\n",
    "#### - = Markov chain\n",
    "#### - 모든 가능한 상태는 유한한 state space  셋으로 설정 \n",
    "- 상태는 이미 지정된 법칙에 따라서만 변화 '나'는 상태에 영향을 끼칠 수 없음 \n",
    "\n",
    "\n",
    "#### - 관찰은 상태의 연속 \n",
    "- 우리는 현재의 날씨를 sunny나 rainy로 관찰 (하나의 state)\n",
    "- 시간의 흐름에 따른 일련의 관찰은 [sunny, sunny, rainy, sunny...] (history)\n",
    "\n",
    "\n",
    "#### - Markov property를 만족해야 함 \n",
    "- 다음상태는 현재상태에만 기반한다 \n",
    "- 미래 상태를 결정하는데, 현재 상태만 고려 (history 또는 N개의 step을 고려하지 않음)\n",
    "\n",
    "\n",
    "#### - Transition matrix \n",
    "- 모양: (행(j), 열(i)) : (상태 개수, 상태 개수)\n",
    "- 값: i에서 j로 상태가 전환될 확률 \n",
    "- 각 상태의 전환 분포는 시간이 지나도 변하지 않음(stationary)\n",
    "- 예) \n",
    "    - 날씨 \n",
    "<img src=\"./image/sunnny_rainy.PNG\" width=200>\n",
    "<img src=\"./image/mp.png\">\n",
    "\n",
    "\n",
    "    - office work \n",
    "<img src=\"./image/coffee_home.PNG\" width=200>\n",
    "<img src=\"./image/mp2.png\" width=400>\n",
    "\n",
    "### 2. Markov reward process\n",
    "#### - Markov process + reward(scalar)\n",
    "- 모든 관찰은 각 전환과 함께 보상을 가짐 \n",
    "\n",
    "#### - discount factor & return \n",
    "$$G_t=R_{t+1} + \\gamma R_{t+2} + ... = \\sum^{\\infty}_{k=0}\\gamma^kR_{t+k+1}$$\n",
    "\n",
    "- return = 이어지는 보상들의 총 합 \n",
    "- discount factor ($\\gamma$): 에이전트의 통찰력(선견지명)\n",
    "    - 강화학습에서 주요 파라미터\n",
    "    - 1: return은 모든 이어지는 보상의 총 합\n",
    "    - 0: return은 즉시 보상 \n",
    "    \n",
    "#### - value of state \n",
    "- reward는 적용(practice)에서 유용하지 못함 \n",
    "    - 모든 세부적인 경우에대해 보상을 설정하는 것은 같은 상태에서조차 값이 다를 수 있음 \n",
    "    \n",
    "- 어떤 상태에서든 수학적 예측 보상 계산 \n",
    "- 상태s가 얼마나 좋은지 \n",
    "\n",
    "$$V(s) = E[G | S_t = s]$$\n",
    "\n",
    "    - 모든 상태 s에서 V(s)는 MRP를 통해 얻을 평균(추정) 보상 \n",
    "   \n",
    "- 예)\n",
    "    - office work\n",
    "<img src=\"./image/mrp.png\" width=300>\n",
    "    - V(chat) = -1 * 0.5 + 2 * 03 + 1 * 0.2 = 0.3\n",
    "\n",
    "    \n",
    "### 3. Markov Decision Process\n",
    "#### - Markov reward process + action(A)\n",
    "- action space가 추가됨 \n",
    "\n",
    "\n",
    "#### - 3차원 transition matrix \n",
    "<img src=\"./image/old_mdp.png\" width=300>\n",
    "<img src=\"./image/mdp.png\" width=300>\n",
    "\n",
    "- 현재 상태, 다음 상태, 액션 \n",
    "\n",
    "\n",
    "- 이제 에이전트는 소극적으로 상태전이를 관찰하지 않고 모든 상황에서 수행할 액션을 활발하게 선택할 것 \n",
    "    - i에서 액션 k를 수행하여 j로 전환될 확률 \n",
    " \n",
    " \n",
    "- 이제 에이전트의 보상은 state, action으로부터 영향을 받음 \n",
    "\n",
    "#### - policy \n",
    "- MDP에서 가장 중요한 요소 \n",
    "- 직관적 정의: 에이전트의 행동을 조절하는 규칙의 셋\n",
    "\n",
    "\n",
    "$$\\pi(a | s) = P[A_t = a | S_t = s]$$\n",
    "\n",
    "\n",
    "- 모근 가능한 상태에 대한 액션에 따른 확률 분포 \n",
    "- MDP에서 policy가 고정이라면, MRP가 됨 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
