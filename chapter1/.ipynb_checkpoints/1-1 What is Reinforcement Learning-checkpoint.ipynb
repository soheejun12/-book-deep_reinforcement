{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter1. What is Reinforcement Learning?\n",
    "\n",
    "#### - 이번장에서는, \n",
    "- 다른 기계학습 기법과 강화학습\n",
    "- 주요 강화학습 구조들 \n",
    "- 강화학습의 이론적 토대: MDP\n",
    "\n",
    "\n",
    "## Learning – supervised, unsupervised, and reinforcement\n",
    "\n",
    "### Supervised learning\n",
    "- 데이터 쌍이 주어졌을 때 입력과 출력을 mapping하는 함수를 자동으로 생성하는 기법 \n",
    "- 예)\n",
    "    - text classification\n",
    "    - image classification, object location\n",
    "    - regression problems\n",
    "    - sentiment analysis \n",
    "    \n",
    "### Unsupervised learning\n",
    "- 레이블이 없는 데이터 사용 \n",
    "- 데이터 세트의 숨겨진 구조 파악이 목적 \n",
    "- 예) \n",
    "    - clustring \n",
    "    - Generative Adversarial Networks (GANs)\n",
    "\n",
    "\n",
    "### Reinforcement learning\n",
    "#### - 머신러닝의 한분야\n",
    "#### - 시간(extra dimension)을 고려하여 최적의 결정을 자동적으로 학습하여 문제를 해결하는 것\n",
    "#### - supervised learning에서 개발된 여러 방법들을 다른 방식을 적용해야 함 \n",
    "#### - 미로 예시 \n",
    "<img src=\"./image/maze.png\" width=300>\n",
    "\n",
    "- 총 보상의 합이 가장 크도록 액션을 수행하는 에이전트 \n",
    "    - 에이전트: 쥐 \n",
    "    - 액션: 좌, 우, 직진 \n",
    "    - 보상: 음식 먹기(양), 전기 통하기(음) \n",
    "    \n",
    "- 더 많은 음식을 모으고, 덜 적게 감전 당하도록 학습 \n",
    "\n",
    "#### - 강화학습이 까다로운 이유 \n",
    "- 관찰 (observation)\n",
    "    - 강화학습은 에이전트의 행동에 근거 \n",
    "    - 에이전트의 행동은 환경에 영향을 미침 \n",
    "    - 하지만 환경은 그들이 잘못한 것이나 더 좋은 결과를 얻으려면 어떻게 해야하는지 알려주지 않음 \n",
    "    - 부정적인 보상만을 줌 \n",
    "    - 방법을 가르쳐주지는 않고 결과만 제공 \n",
    "\n",
    "- exploration(탐험), exploitation(탐색) dilemma\n",
    "    - 반드시 정책만을 따라 행동하지 않아야 함 \n",
    "    - 다른 행동은 다른 중요한 결과를 낼 수도 있음 \n",
    "    - 둘의 적절한 조화 필요 \n",
    "    \n",
    "\n",
    "## RL formalisms and relations\n",
    "\n",
    "### 강화학습의 주요 요소 \n",
    "<img src=\"./image/rl.png\" width=300>\n",
    "          \n",
    "#### - 객체: 에이전트, 환경 \n",
    "#### - 연결: 액션, 보상, 관찰 \n",
    "\n",
    "### Reward \n",
    "#### - 환경이 에이전트에게 제공하는 첫번째 채널 \n",
    "#### -환경으로부터 주기적으로 얻는 스칼라 값\n",
    "#### -긍정적(양의 보상)이거나 부정적(음의 보상)\n",
    "#### - 목적\n",
    "- 에이전트의 행동에대한 피드백 \n",
    "- 에이전트는 가장 큰 축적보상을 얻기위해 학습됨 \n",
    "\n",
    "### The Agent \n",
    "#### - 환경과 상호작용하는 객체 \n",
    "- 관찰\n",
    "- 액션을 실행 \n",
    "- 보상을 획득 \n",
    "\n",
    "### The Environment \n",
    "#### - 에이전트 외부의 모든 것 \n",
    "#### - 환경과의 소통 \n",
    "- 환경으로부터 얻은 보상\n",
    "- 에이전트가 환경에 실행하는 액션 \n",
    "- 에이전트가 환경으로부터 받는 관찰(정보) \n",
    "\n",
    "\n",
    "### Actions\n",
    "#### - 에이전트가 환경 내에서 할 수 있는 행동 \n",
    "#### - 이산적(Discrete) 액션 \n",
    "- 왼쪽, 오른쪽 이동 \n",
    "- 액션 간의 관계가 상호배타적 \n",
    "#### - 연속적(Continuous) 액션 \n",
    "- 값을 가지는 행동 \n",
    "- 자동차의 바퀴를 조정할 떄의 각도, 방향 \n",
    "\n",
    "\n",
    "###  Observations\n",
    "#### - 환경이 에이전트에게 제공하는 두번째 채널 \n",
    "#### - 에이전트에게 상황이 어떻게 되어가는지 알려주는 정보 \n",
    "#### - 환경의 상태와 관찰을 구분 \n",
    "- 환경의 상태 (state of environment)\n",
    "    - 환경에서 측정할 수 있는 모든 정보 \n",
    "    \n",
    "- 관찰 (observationn)\n",
    "    - 에이전트가 볼 수 있는 것 \n",
    "  \n",
    "### 강화학습 관련 영역 \n",
    "<img src=\"./image/area.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
