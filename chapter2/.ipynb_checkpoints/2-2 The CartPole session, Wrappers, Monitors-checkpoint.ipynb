{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CartPole session\n",
    "### 환경\n",
    "#### - \"classic control\" 환경 \n",
    "#### - 목표 \n",
    "- 막대기가 부착 된 카트를 제어하는 것\n",
    "- 왼쪽, 오른쪼긍로 움직여서 균형을 잡아 막대기를 떨어트리지 않는 것\n",
    "    \n",
    "#### - 관찰 (4개)\n",
    "- 스틱의 질량 중심 x 좌표(스틱의 각도)\n",
    "- 스틱의 속도 \n",
    "- 카트의 위치 \n",
    "- 카트의 속도\n",
    "\n",
    "#### - 보상 \n",
    "- 매 단계마다 제공\n",
    "- 에피소드는 스틱이 떨어질 때까지 진행 \n",
    "- 누적 된 보상을 얻으려면 스틱이 떨어지지 않도록 플랫폼의 균형을 유지 필요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-01-20 00:04:28,067] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "e = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00980206, -0.0118    , -0.02252819,  0.00181933])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = e.reset() \n",
    "obs #(스틱 각도, 스틱 속도, 카트 위치, 카트 속도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space  #0, 1: 왼쪽, 오른쪽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space #[-inf, inf] 범위의 크기가 4인 벡터 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 액션 0 실행\n",
    "    - 반환: (새로운 관찰, 보상, 에피소드 종료 여부, 추가 정보)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00543422, -0.40138584, -0.01674561,  0.57281528]), 1.0, False, {})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space.sample() #액션 무작위 샘플링 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.91308335e-01, 2.34295114e+38, 2.99871819e-01, 2.36327273e+38])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.observation_space.sample() #관찰 무작위 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - The random Cartpole agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-01-20 00:11:08,191] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 19 steps, total reward 19.00\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    \n",
    "    obs = env.reset() #환경 초기화 \n",
    "    #우리의 에이전트는 확률적이기 때문에 사용되지 않을 것? (하나의 에피소드만 수행?) \n",
    "  \n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        obs, reward, done, _ = env.step(action) #액션 수행 \n",
    "        \n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -  대부분의 Gym환경에는 보상경계 존재 \n",
    "- 보상 경계 : 100회 연속 에피소드 중 에이전트가 얻게 되는 평균 보상 \n",
    "- CartPole: 195 (평균적으로 에이전트가 195번의 timestep동안 막대 균형을 유지해야 함을 의미)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The extra Gym functionality – wrappers and monitors\n",
    "\n",
    "#### : 알고리즘을 더 쉽게하고 코드를 더 깨끗하게 만드는 함수들\n",
    "\n",
    "### Wrappers\n",
    "#### - 환경 기능을 확장할 때, \n",
    "- 예)\n",
    "    - 환경의 관찰을 일부 버퍼에 누적하고 에이전트에 N 개의 마지막 관찰을 제공하려는 경우 \n",
    "    - 동적 컴퓨터 게임에서, 게임 상태에 대한 전체 정보가 하나의 프레임만으로는 충분하지 않은 경우 \n",
    "\n",
    "    - 에이전트가 더 잘 이해할 수 있도록 이미지 픽셀을 자르거나 전처리하는 경우 \n",
    "    - 보상 점수를 정규화하려는 경우\n",
    "    \n",
    "#### -기존 환경을 \"감싸고(wrap)\"추가 작업을 추가하고 싶을 때, \n",
    "\n",
    "////\n",
    "#### -Wrapper 클래스는 Env클래스를 상속받음 \n",
    "- 생성자는 \"wrapped\"할 환경의 인스턴스를 유일한 인자로 받음 \n",
    "- 추가 기능을 추가하려면 step () 또는 reset ()과 같이 확장할 메소드 재정의 필요 \n",
    "    -  superclass의 원래 메소드를 호출\n",
    "    \n",
    "#### - Wrapper 클래스의 하위 클래스 \n",
    "- 구체적인 요구 사항을 처리하기 위해 특정 정보 부분만 필터링 \n",
    "- ObservationWrapper \n",
    "    - 부모클래스의 관찰(obs) 메소드를 재정의 필요 \n",
    "    - obs 인수는 wrapped된 환경에서의 관찰\n",
    "    - 에이전트에 제공될 관찰 반환 \n",
    "    \n",
    "    \n",
    "- RewardWrapper \n",
    "    - 에이전트에게 주어진 보상을 수정할 수 있는 보상(rew)메서드 표현 \n",
    "    \n",
    "    \n",
    "- ActionWrapper \n",
    "    - wrapped된 환경으로 전달된 액션을 에이전트에 맞게 조정할 수있는 action(act) 메소드를 override 필요 \n",
    "    -  \"exploration(탐험)/exploitation(탐색) 문제\" 해결 시 사용 가능 \n",
    "        - 가끔 무작위로 액션 수행 \n",
    "        - ActionWrapper 클래스를 통해 간단하고 실용적이게 구현 가능 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, epsilon=0.1):\n",
    "        super(RandomActionWrapper, self).__init__(env)\n",
    "        #부모의 __init__ 메소드를 호출\n",
    "        \n",
    "        self.epsilon = epsilon #엡실론 10%       \n",
    "        \n",
    "    \n",
    "    def action(self, action):\n",
    "        if random.random() < self.epsilon:\n",
    "            print(\"Random!\")\n",
    "            \n",
    "            return self.env.action_space.sample()\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 부모class에서 오버라이드하여 에이전트의 행동을 조정하는 방법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-01-20 00:39:31,568] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward got: 9.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = RandomActionWrapper(gym.make(\"CartPole-v0\"))\n",
    "    \n",
    "    #원래의 CartPole 대신 일반 Env 인스턴스로 wrapper 사용\n",
    "    #Wrapper 클래스는 Env 클래스를 상속, 동일한 인터페이스를 \n",
    "    #포함하므로 원하는 조합으로 래퍼를 중첩 가능 \n",
    "    \n",
    "    obs = env.reset()\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while True:\n",
    "        obs, reward, done, _ = env.step(0)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Reward got: %.2f\" % total_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor\n",
    "\n",
    "- Wrapper와 같이 구현됨 \n",
    "- 에이전트의 성능에 대한 정보를 파일로 기록 \n",
    "- Monitor 클래스의 결과를 https://gym.openai.com 웹 사이트에 업로드하고 다른 사람들의 결과와 비교하여 에이전트의 위치 확인 가능 \n",
    "- 그러나 , 2017 년 8 월 말, OpenAI는 업로드 기능을 종료, 모든 결과를 동결하기로 결정. \n",
    "- 환경 내에서 에이전트의 움직임 확인용으로 사용됨 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CartPole환경에 Monitor 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-01-20 00:42:20,604] Making new env: CartPole-v0\n",
      "[2019-01-20 00:42:20,621] Clearing 7 monitor files from previous run (because force=True was provided)\n",
      "[2019-01-20 00:42:20,627] Starting new video recorder writing to /home/sohee/semina/Deep Reinforcement/chapter2. OpernAIGym/recording/openaigym.video.0.5654.video000000.mp4\n",
      "[2019-01-20 00:42:22,867] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/sohee/semina/Deep Reinforcement/chapter2. OpernAIGym/recording')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode done in 11 steps, total reward 11.00\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env = gym.wrappers.Monitor(env, \"recording\", force=True)\n",
    "    #gym.wrappers.Monitor(환경, 결과 기록 디렉토리)\n",
    "    #존재하지 않는 디렉토리 사용 필요 \n",
    "    #존재하는 디렉토리를 사용하려면 force=True 인자 사용 \n",
    "    \n",
    "    total_reward = 0.0\n",
    "    total_steps = 0\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(\"Episode done in %d steps, total reward %.2f\" % (total_steps, total_reward))\n",
    "    env.close()\n",
    "    env.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 과정을 비디오로 기록하기 위해 요구되는 조건 책에 저술 \n",
    "    - 현재 환경(virtual box-ubuntu)에서는 sudo-apt get install ffmpeg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
