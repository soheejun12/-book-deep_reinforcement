{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. OpenAI Gym \n",
    "\n",
    "#### - 이번장에서는 \n",
    "- OpenAI Gym API의 기본 \n",
    "- 전반적인 개념의 이해를 위해 무작위로 행동하는 에이전트 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The anatomy of the agent \n",
    "\n",
    "### 강화학습 관점의 여러 요소 \n",
    "\n",
    "#### - Agent(에이전트)\n",
    "- 행동 역할을하는 사람 또는 사물\n",
    "- 정책을 구현하는 코드\n",
    "- 기본적으로 이 정책은 주어진 관찰에 대해 매 시간 단계마다 어떤 행동이 필요한지 결정\n",
    "\n",
    "#### - Environment(환경)\n",
    "- 에이전트의 외부에 존재\n",
    "- 관찰을 제공하고 보상을 제공\n",
    "- 행동에 따라 state(상태)가 변화됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 구현 예시 \n",
    "- 에이전트의 액션에 상관없이 제한된 step 개수에따른 보상을 제공하는 환경 \n",
    "- 활용성이 없는 환경 \n",
    "- 환경과 에이전트 클레스의 세부적인 method들에 집중 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 환경 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"환경의 내부상태 초기화\"\"\"\n",
    "    def __init__(self):\n",
    "        self.steps_left = 10\n",
    "        #에이전트가 환경과 상호작용 할 수 있는 최대 횟수 \n",
    "  \n",
    "    \"\"\"현재 환경의 관찰 내용을 에이전트로 리턴\"\"\"\n",
    "    def get_observation(self):\n",
    "        return [0.0, 0.0, 0.0]\n",
    "        #이 예제에서 환경은 기본적으로 내부 상태가 없음 --> 관측 벡터는 항상 0\n",
    "    \n",
    "    \"\"\"에이전트가 실행할 수 있는 액션에 대해 조회\"\"\"\n",
    "    def get_actions(self):\n",
    "        return [0, 1]\n",
    "        #에이전트가 할 수 있는 액션은 시간에 따라 변할 수도 있고 변하지 않을 수도 있음\n",
    "        #(예 : TicTacToe 게임의 모든 위치에서 모든 이동이 가능하지는 않음)\n",
    "        #이 예제는 액션 2개 (0, 1로 인코딩)\n",
    "        \n",
    "    \"\"\"에피소드의 종료 여부를 에이전트에게 전달\"\"\" \n",
    "    def is_done(self):\n",
    "        return self.steps_left == 0  # 에피소드가 마지막 0가 되었을 때\n",
    "        #에이전트 상호 작용은 에피소드라는 일련의 단계로 구분됨 \n",
    "        #무한, 유한 시나리오를 모두 다루기위해 환경은 에피소드 종료를 감지하고 에이전트와 상호작용을 종료 \n",
    "        \n",
    "    \"\"\"에이전트의 액션 처리\"\"\"\n",
    "    \"\"\"액션에 대한 보상 반환\"\"\"\n",
    "    def action(self, action):\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\") #종료된 에피소드는 무시 \n",
    "            \n",
    "        self.steps_left -= 1  #액션을 수행할 때마다 step 1 감소 \n",
    "        \n",
    "        return random.random() #보상 (0-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 에이전트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"에피소드 동안 축적된 총보상 카운터 초기화\"\"\"\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "   \n",
    "    \"\"\"에이전트의 액션을 환경에 수행하고 보상 획득\"\"\"\n",
    "    def step(self, env):\n",
    "        current_obs = env.get_observation() #관찰\n",
    "        actions = env.get_actions() #액션 확인\n",
    "        reward = env.action(random.choice(actions)) #액션 수행 \n",
    "        self.total_reward += reward #보상 획득 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward got: 5.4606\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = Environment() #환경 생성 \n",
    "    agent = Agent() #에이전트 생성 \n",
    "\n",
    "    while not env.is_done(): #True이면 break \n",
    "        agent.step(env) \n",
    "\n",
    "    print(\"Total reward got: %.4f\" % agent.total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\n"
     ]
    }
   ],
   "source": [
    "a = True\n",
    "while not a:\n",
    "    print(\"i\")\n",
    "    \n",
    "print(\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대부분의 강화학습 알고리즘의 패턴은 비슷 \n",
    "- 이미 존재하는 framework들이 있지만 그것들을 보기 전에 자신만의 개발 환경을 준비해보자 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware and software requirements\n",
    "\n",
    "### 책 코드 구현 환경 \n",
    "- 책의 예제는 Python 버전 3.6을 사용\n",
    "- 책에서 사용할 외부 라이브러리\n",
    "\n",
    "    - NumPy: 과학 연산과 행렬 연산 및 함수를 구현하기위한 라이브러리.\n",
    "    - OpenCV Python bindings: (실시간)이미지 프로세싱을위한 많은 기능을 제공하는 컴퓨터 비전 라이브러리\n",
    "    - Gym: OpenAI가 개발하고 유지 관리하는 RL 프레임 워크이며, 통일 된 방법으로 다양한 환경에서 통신 가능 \n",
    "    - PyTorch: 유연하고 표현력이 깊은 DL (Deep Learning) 라이브러리. \n",
    "    - PTAN(PyTorchAgentNet): 현대 딥 RL 메서드와 빌딩 블록(building blocks)을 지원하기 위해 작성한 gym의 확장된 오픈소스 \n",
    "    \n",
    "- numpy==1.14.2\n",
    "- atari-py==0.1.1\n",
    "- gym==0.10.4\n",
    "- ptan==0.3\n",
    "- opencv-python==3.4.0.12\n",
    "- scipy==1.0.1\n",
    "- torch==0.4.0\n",
    "- torchvision==0.2.1\n",
    "- tensorboard-pytorch==0.7.1\n",
    "- tensorflow==1.7.0\n",
    "- tensorboard==1.7.0\n",
    "\n",
    "### GPU 환경 권장 \n",
    "### Linux, mac0S 권장 \n",
    "### PyTorch 사용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym API \n",
    "#### :Gym Python 라이브러리: OpenAI (www.openai.com)에 의해 개발 \n",
    "### 목표\n",
    "#### - 통합 인터페이스를 사용하여 RL 실험을위한 다양한 환경을 제공\n",
    "- 라이브러리의 중심 클래스는 Env라는 환경\n",
    "    - 환경의 기능에 대해 필요한 정보를 제공하는 여러 가지 메소드과 필드를 제공\n",
    "    - 높은 수준에서 모든 환경은 다음과 같은 정보 및 기능을 제공\n",
    "\n",
    "        - 환경에서 실행될 수있는 액션 세트. 이산적, 연속적 액션을 모두 지원\n",
    "        - 환경이 에이전트에게 제공하는 관찰의 모양과 경계.\n",
    "        - step(): 현재 관찰, 보상 및 종료여부를 반환하는 액션을 실행\n",
    "        - reset():환경을 초기 상태로 되돌리고 첫 번째 관찰을 반환  \n",
    "        \n",
    "### 환경의 구성요소 \n",
    "#### - Action space \n",
    "- 이산적/연속/둘 다 \n",
    "    - 이산 액션: 고정된 액션\n",
    "        - 단 하나의 액션만 가능 --> 상호 배타적\n",
    "        - 그리드의 방향 (왼쪽, 오른쪽, 위, 아래)\n",
    "        - 푸시 버튼 (누르거나 떼기)\n",
    "       \n",
    "    - 연속 액션: 값 (attached value)\n",
    "        - 액션이 가질 수있는 가치의 경계(범위)를 포함 \n",
    "        - 스티어링 휠 (특정 각도로 회전) (-720 - 720)\n",
    "        - 가속 페달 (다른 수준의 힘으로 가속 가능) (0 - 1)\n",
    "       \n",
    "    - 동시에도 가능 \n",
    "        - 여러 버튼을 동시에 누르거나 휠을 조종하고 두 개의 페달 (브레이크 및 가속기)을 누르는 등 \n",
    "        - 특수 '컨테이너 클래스'를 정의 필요: 여러 액션 공간을 하나의 통합 액션으로 중첩 가능 \n",
    "        \n",
    "#### - Observation space\n",
    "\n",
    "- 모든 time step에서 보상과 함께 환경이 에이전트에게 제공하는 정보 조각\n",
    "- 종류 \n",
    "    - 여러 개의 뭉치(bunch)처럼 간단\n",
    "    - 여러 카메라의 컬러 이미지가 포함 된 여러 다차원 텐서\n",
    "    - 이산적\n",
    "        - 예) 두 가지 상태, 불리언 값(0 or 1)으로, 켜짐 또는 꺼짐으로된 전구\n",
    "\n",
    "<img src=\"./image/space.png\" width=400>\n",
    "\n",
    "#### - 기본 추상 클래스 space\n",
    "- sample () \n",
    "    - space에서 임의의 샘플을 반환\n",
    "    - 임의의 액션 선택이 필요한 action space에서 유용 \n",
    "- contains (x) \n",
    "    - 인자 x가 space의 도메인에 속하는지 체크 \n",
    "    - Gym 내부에서 에이전트의 정상 작동 여부 확인에 사용 \n",
    "\n",
    "- 두 메소드는 추상적인 클래스, 하위 클래스인 Space 클래스에서 다시 구현됨 \n",
    "\n",
    "#### - Discrete 클래스\n",
    "- 0부터 n-1까지 번호가 매겨진 상호 배타적인 항목 집합\n",
    "- n: 액션 수 \n",
    "\n",
    "#### - Box 클래스\n",
    "- 간격이 유리수로 된 n 차원 텐서 표현 (low, high)\n",
    "- 예) 0.0과 1.0 사이의 하나의 단일 값을 가진 가속 페달은 'Box(low = 0.0, high = 1.0, shape = (1), dtype = np.float32)'로 인코딩 \n",
    "    - shape(): 텐서의 차원 \n",
    "    - dtype 파라미터: 공백의 값 유형을 지정\n",
    "    \n",
    "- 예) 아타리 screen observation\n",
    "    - 210x160 크기의 RGB 이미지\n",
    "    - Box (low = 0, high = 255, shape = (210, 160, 3), dtype = np.uint8)\n",
    "    \n",
    "#### - Tuple 클래스\n",
    "- 여러 개의 Space 클래스 인스턴스를 함께 결합 가능\n",
    "- 우리가 원하는 복잡성을 가진 action과 observation spaces 생성 가능 \n",
    "\n",
    "### 주요 함수 \n",
    "\n",
    "#### - reset()\n",
    "- 인수 없음 \n",
    "- 환경을 초기 상태로 재설정하고 초기 관찰 반환 \n",
    "- 환경을 생성 후에는 reset ()을 호출 필요 \n",
    "- 환경과의 소통 종료 가능  (에피소드 종료 시)\n",
    "\n",
    "#### - step () \n",
    "- 환경 기능의 핵심 요소\n",
    "- 한 번의 호출로 여러 가지 작업을 수행\n",
    "    1. 다음 단계에서 실행할 액션을 환경에 전달 \n",
    "    2. 액션 수행 후 환경으로부터 새로운 관찰 획득 \n",
    "    3. 에이전트는 보상 획득\n",
    "    4. 에피소드의 종료 여부 확인 \n",
    "\n",
    "- 반환 \n",
    "    - (관찰, 보상, 완료, 추가 정보)\n",
    "    - observation: 관측 데이터(NumPy 벡터 또는 행렬)\n",
    "    - reward: 보상의 실수 값\n",
    "    - done: 에피소드가 끝나면 False → True (불리언)\n",
    "    - extra_info : 환경에 관한 추가 정보가 있는 환경 특정 항목 (보통 무시)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the environment\n",
    "\n",
    "### Gym에서 제공하는 환경 \n",
    "#### - 모든 환경에는 (환경이름-v(N)) 형식의 고유 한 이름 존재 \n",
    "- (N = 버전 : 일부 버그가 환경에서 수정되거나 다른 주요 변경 사항이 수행되는 경우)\n",
    "- 환경 생성 시  make함수를 사용 ( make(env_name))\n",
    "\n",
    "#### - Gym version 0.9.3에는 777 개의 다른 이름의 환경 존재 \n",
    "- 모든 버전의 환경이 포함\n",
    "- 각각은 고유 한 환경이 아님 \n",
    "- 동일한 환경에서 설정 및 관측 공간이 서로 다를 수 있음 \n",
    "    - 예) Atari 게임의 Breakout (12개)\n",
    "\n",
    "        - Breakout-v0, Breakout-v4 : 무작위로 초기 위치와 볼의 방향을 가진 원조 브레이크 아웃\n",
    "        - BreakoutDeterministic-v0, BreakoutDeterministic-v4 : 볼의 초기 배치와 속도 벡터가 동일한 Breakout\n",
    "        - BreakoutNoFrameskip-v0, BreakoutNoFrameskip-v4 : 모든 프레임이 에이전트에 표시되는 Breakout\n",
    "        - Breakout-ram-v0, Breakout-ram-v4 : 화면 픽셀 대신 전체 Atari 에뮬레이션 메모리 (128 바이트)를 관찰하는 Breakout. (Emulation: 다른 컴퓨터의 기계어 명령대로 실행할 수 있는 기능)\n",
    "        - Breakout - ramDeterministic-v0, Breakout - ramDeterministic-v4\n",
    "        - Breakout-ramNoFrameskip-v0, Breakout-ramNoFrameskip-v4\n",
    "\n",
    "#### - 중복을 제외하고도 Gym 0.9.3에는 116 개의 독특한 환경 존재 \n",
    "- Classic control problems \n",
    "    - 최적 제어 이론 및 RL 논문에서 벤치 마크 또는 설명서로 사용되는 toy task\n",
    "    - 단순하고, 낮은 차원의 관찰과 행동 공간을 가짐 \n",
    "    - 알고리즘을 구현할 때 빠른 점검에 유용\n",
    "    - \"강화학습을 위한 MNIST\"\n",
    "    \n",
    "    \n",
    "- Atari 2600 \n",
    "    - 1970 년대 고전 게임 플랫폼의 게임 (63개)\n",
    "    \n",
    "    \n",
    "- Algorithmic \n",
    "    - 작은 계산 작업(관찰된 시퀀스 복사/숫자 추가)을 목표로하는 문제\n",
    "    \n",
    "    \n",
    "- Board games \n",
    "    - Go, Hex 게임\n",
    "    \n",
    "  \n",
    "- Box2D \n",
    "    - 보행이나 자동차 제어를 배우기 위해 Box2D 물리 시뮬레이터를 사용하는 환경\n",
    "    \n",
    "    \n",
    "- MuJoCo \n",
    "    - : 이것은 몇 가지 연속 제어 문제에 사용되는 또 다른 물리 시뮬레이터\n",
    "    \n",
    "    \n",
    "- parameter tuning\n",
    "    - 신경망 파라미터 최적화하에 사용 \n",
    "    \n",
    "    \n",
    "- toy text \n",
    "    - 간단한 그리드-세계 텍스트 환경\n",
    "    \n",
    "    \n",
    "- PyGame \n",
    "    - PyGame 엔진을 사용하여 구현된 여러 환경\n",
    "    \n",
    "    \n",
    "- Doom\n",
    "    - ViZdoom에 구현 된 9 개의 미니 게임\n",
    "    \n",
    "#### - 전체 환경 \n",
    "- https://gym.openai.com/envs \n",
    "- 프로젝트의 GitHub 저장소에있는 wiki 페이지에서 제공 \n",
    "\n",
    "#### - OpenAI Univers\n",
    "- 더 광범위한 환경 \n",
    "- Gym의 확장 api 사용 \n",
    "- 가상 시스템에 대한 일반 커넥터를 제공하고 Flash 및 기본 게임, 웹 브라우저 및 기타 실제 응용 프로그램을 실행하는 환경 \n",
    "- https://github.com/openai/universe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
